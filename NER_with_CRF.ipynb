{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_with_CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQlaVHgIVXWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN6pXdAAVomr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"drive/My Drive/NER/ner_dataset.csv\"   # setting path for the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmbkZ_8KV5Ag",
        "colab_type": "code",
        "outputId": "c5000c5d-3127-440b-e51b-40894d8d0f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df = pd.read_csv(data_path, encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
        "#reading the data as a pandas dataframe\n",
        "df.head()\n",
        "# The task is to map each Word to correct Tag"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1          NaN             of   IN   O\n",
              "2          NaN  demonstrators  NNS   O\n",
              "3          NaN           have  VBP   O\n",
              "4          NaN        marched  VBN   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pRZLOtkV8fZ",
        "colab_type": "code",
        "outputId": "40084afc-f803-4aee-abc0-6c880b254fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# As the data has sentence id for the first word for the sentence only, and NaN for all other words\n",
        "# We fill all the NaNs with the previous value.\n",
        "# This is helpful later when we use groupby to extract data.\n",
        "df = df.fillna(method='ffill')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O\n",
              "3  Sentence: 1           have  VBP   O\n",
              "4  Sentence: 1        marched  VBN   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK0BTnXfc4dk",
        "colab_type": "code",
        "outputId": "01a720ae-31c5-46ec-edff-a432c990be05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1048575 entries, 0 to 1048574\n",
            "Data columns (total 4 columns):\n",
            "Sentence #    1048575 non-null object\n",
            "Word          1048575 non-null object\n",
            "POS           1048575 non-null object\n",
            "Tag           1048575 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 32.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FQygeHUWLjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we want to extract (Word, Tag) pairs for \"each sentence\".\n",
        "# The following extract_pairs function will extract all the (Word , Tag) pairs regardless of the sentence.\n",
        "# So we apply it to pandas groupby\n",
        "def extract_pairs(df):\n",
        "    return [(word.lower() , tag.lower()) for word, tag in zip(df[\"Word\"].values.tolist() , df[\"Tag\"].values.tolist())]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bksoHrdCXOVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We group by sentence # and then make pairs for each individual sentence.\n",
        "sentences_pairs = df.groupby(\"Sentence #\").apply(extract_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMwRKQHQXl7J",
        "colab_type": "code",
        "outputId": "aa18e4f5-2ee9-4e95-89cc-f06bf7a0b8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# So for first sentence the (Word,Tag) pairs look like this\n",
        "sentences_pairs[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('thousands', 'o'),\n",
              " ('of', 'o'),\n",
              " ('demonstrators', 'o'),\n",
              " ('have', 'o'),\n",
              " ('marched', 'o'),\n",
              " ('through', 'o'),\n",
              " ('london', 'b-geo'),\n",
              " ('to', 'o'),\n",
              " ('protest', 'o'),\n",
              " ('the', 'o'),\n",
              " ('war', 'o'),\n",
              " ('in', 'o'),\n",
              " ('iraq', 'b-geo'),\n",
              " ('and', 'o'),\n",
              " ('demand', 'o'),\n",
              " ('the', 'o'),\n",
              " ('withdrawal', 'o'),\n",
              " ('of', 'o'),\n",
              " ('british', 'b-gpe'),\n",
              " ('troops', 'o'),\n",
              " ('from', 'o'),\n",
              " ('that', 'o'),\n",
              " ('country', 'o'),\n",
              " ('.', 'o')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z7qTA5_ZTwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets do some exploration \n",
        "# Counting unique words and making word2index index2word , tag2index, index2tag\n",
        "def make_dicts(sentences_pairs):\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    tag2index = {}\n",
        "    index2tag = {}\n",
        "    # We add speacial symbol <pad> in our word list\n",
        "    word2index[\"<pad>\"] = 0\n",
        "    index2word[0] = \"<pad>\"\n",
        "    word_index = 1\n",
        "    # We add <BOT> and <EOT> symbols ie Beginning of tag and End Of Tag along with <PADT> ie Pad Tag\n",
        "    tag2index[\"<PADT>\"] = 0\n",
        "    tag2index[\"<BOT>\"]  = 1\n",
        "    tag2index[\"<EOT>\"] = 2\n",
        "    index2tag[0] = \"<PADT>\"\n",
        "    index2tag[1] = \"<BOT>\"\n",
        "    index2tag[2] = \"<EOT>\"\n",
        "    tag_index = 3\n",
        "    # now indexing\n",
        "    for sent in sentences_pairs:\n",
        "        for pair in sent:\n",
        "            word = pair[0]\n",
        "            tag = pair[1]\n",
        "            if(word not in word2index):\n",
        "                word2index[word] = word_index\n",
        "                index2word[word_index] = word\n",
        "                word_index = word_index + 1\n",
        "            if(tag not in tag2index):\n",
        "                tag2index[tag] = tag_index\n",
        "                index2tag[tag_index] = tag\n",
        "                tag_index += 1\n",
        "    return word2index , index2word , tag2index , index2tag\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMDLVLUAczYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gettting dictionaries, we will beusing this for all further indexing\n",
        "word2index , index2word , tag2index , index2tag  =make_dicts(sentences_pairs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKEmQbjhc8j8",
        "colab_type": "code",
        "outputId": "daf9756d-4db9-4b1e-b84c-3115291c8e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# Here is how tag2indexlooks like....\n",
        "tag2index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<BOT>': 1,\n",
              " '<EOT>': 2,\n",
              " '<PADT>': 0,\n",
              " 'b-art': 13,\n",
              " 'b-eve': 18,\n",
              " 'b-geo': 4,\n",
              " 'b-gpe': 5,\n",
              " 'b-nat': 15,\n",
              " 'b-org': 7,\n",
              " 'b-per': 9,\n",
              " 'b-tim': 6,\n",
              " 'i-art': 14,\n",
              " 'i-eve': 19,\n",
              " 'i-geo': 8,\n",
              " 'i-gpe': 16,\n",
              " 'i-nat': 17,\n",
              " 'i-org': 11,\n",
              " 'i-per': 10,\n",
              " 'i-tim': 12,\n",
              " 'o': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu0ug4ILfDu0",
        "colab_type": "code",
        "outputId": "ab14dd23-49f7-4a22-ae98-8e5aecf837d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# here is how these dictionaries work\n",
        "print(word2index[\"i\"])\n",
        "print(index2word[432])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4702\n",
            "refused\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QY8wIaW30fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting all the words into their respective index\n",
        "# So (\"london\" ,\"b-eve\") becomes (324 , 6)\n",
        "def words_to_numbers(data):\n",
        "    new_data = []\n",
        "    for sent in data:\n",
        "        new_sent = []\n",
        "        for pair in sent:\n",
        "            new_sent.append((word2index[pair[0]] ,tag2index[pair[1]]))\n",
        "        new_data.append(new_sent)\n",
        "    return new_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXjw25xE40-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_pairs = words_to_numbers(sentences_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1rAPAICfjvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets split our test and train data\n",
        "test_data_size = int(.2*len(sentences_pairs))\n",
        "# Randomly Shuffling the data\n",
        "random.shuffle(sentences_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIQEGp182N1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = np.array(sentences_pairs[:test_data_size])\n",
        "train_data = np.array(sentences_pairs[test_data_size:])\n",
        "# We convert into numpy array so that its easy to index them"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bVB3GxF2ZQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next lets make a function that feeds batches to out model\n",
        "# I use stostatic sampling here \n",
        "def get_train_batch(data , batch_size):\n",
        "    index = np.random.randint(low = 0 , high = len(data) -1 , size = batch_size)\n",
        "    batch = data[index].tolist()\n",
        "    # We have a random sample of size \"batch_size\"\n",
        "    batch = list(map(lambda x: torch.tensor(x) , batch)) # Converting each batch into tensors\n",
        "    batch = pad_sequence(batch , batch_first= True) # We now pad the shorter sequences with zeros\n",
        "    # batch has shape (batch_size , len of longest_seq , 2), the last dim is for words and tags\n",
        "    # seperating words and tags\n",
        "    X = batch[:,:,0]   # Input data\n",
        "    y = batch[:,:,1]   # Target values\n",
        "    return X.to(dtype=torch.long, device=device),y.to(dtype=torch.long, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTtQ49dl_BFc",
        "colab_type": "text"
      },
      "source": [
        "***Now we can start writting the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owytmvsp3If1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We start writting the CRF model first as I will be using it on top of the main LSTM model\n",
        "# For now assume running LSTM model gives output of size (batch_size , len of longest seq , n_labels)\n",
        "# Notice n_labels here is actually the total number of possible Tags in this case 20."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7F0SDhABNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I have posted links about the CRF linear model in the github project. \n",
        "# WE use CRF model to calculate the loss function\n",
        "class CRF_linear(nn.Module):\n",
        "    def __init__(self,  n_labels):\n",
        "        super().__init__()\n",
        "        # we first define the transition matrix\n",
        "        # Tm[i,j] represents the probablity of going from i th tag to j th\n",
        "        self.n_labels = n_labels \n",
        "        self.transition_matrix = nn.Parameter(torch.rand(self.n_labels , self.n_labels))\n",
        "        # now lets set the probablity of going to any tag from <EOT> almost zero\n",
        "        # similarly prob of going from any tag to <BOT>\n",
        "    #    self.transition_matrix[tag2index[\"<EOT>\"]:] = -10000.0\n",
        "    #    self.transition_matrix[:tag2index[\"<BOT>\"]] = -10000.0\n",
        "        # Large negative number will zero the probablity as we will be using exp.\n",
        "        # Will come back to this later.\n",
        "    def forward(self, predicted , targets):  # Returns the loss\n",
        "        # Predicted size --> (batch_size , len of longest seq , n_labels)\n",
        "        # target size -->    (batch_size , len of longest seq)\n",
        "        # We must now calculate the P(y/X) for this input.\n",
        "        # This requires us to calculate both probablity of current alignment\n",
        "        # and sum of probablity of all possible alignment (partition function)\n",
        "        # Finally our loss is log of (P(current_alignment)/partition function)\n",
        "        return self.partition_optimized(predicted, targets) - self.calc_current_score(predicted , targets)\n",
        "\n",
        "    def calc_current_score(self ,predicted , targets):\n",
        "        batch_size , seq_len , _ = predicted.size()\n",
        "        # Two scores are computed here\n",
        "        # Stand alone probablity score ie what is the probablity for the correct target\n",
        "        # transition score\n",
        "\n",
        "\n",
        "        # computing stand alone probablity score\n",
        "        # gathering the probablity for the correct target.  \n",
        "        prob_score = torch.gather(dim = 2 ,input = predicted , index = targets.unsqueeze(-1))\n",
        "        prob_score =  prob_score.sum(1) # Has shape (batch_size , 1)\n",
        "        prob_score = prob_score.squeeze() # shape (batch_size)\n",
        "\n",
        "        # now computing the transition scores\n",
        "        trans_score = torch.zeros(predicted.size(0)).float().to(device)\n",
        "        # initialize with the transition form <BOT> to first target\n",
        "        trans_score = trans_score  + self.transition_matrix[tag2index[\"<BOT>\"], targets[:,0]]\n",
        "        for i in range(1, seq_len):\n",
        "            trans_score = trans_score + self.transition_matrix[targets[:,i-1] , targets[:,i]]\n",
        "\n",
        "        trans_score = trans_score + self.transition_matrix[targets[:,-1] , tag2index[\"<EOT>\"]]\n",
        "        \n",
        "        # Notice how i am not taking log here, I am already in log space and it has negated the exp\n",
        "        return trans_score + prob_score \n",
        "\n",
        "    def calc_partition_func(self , predicted, targets):\n",
        "        # Running the Forward algo\n",
        "        batch_size , seq_len , _ = predicted.size()\n",
        "        n_labels = self.n_labels \n",
        "\n",
        "        # alpha_iter at every time step calculates the probablity of reaching the given label through all paths\n",
        "        # Remember this is in log space\n",
        "        alpha_iter = predicted[:,0,:] + self.transition_matrix[tag2index[\"<BOT>\"],:].unsqueeze(0)\n",
        "\n",
        "        for i in range(1 , seq_len):  # Iterating over seq_length\n",
        "            store = torch.empty(batch_size , n_labels).to(device)\n",
        "            for j in range(n_labels): # Iterating over tags\n",
        "                # Idea is to store probality of getting to the jth tag in the ith seq\n",
        "                # Then use it to calculate the ith +1 seq\n",
        "                prob_tag = predicted[:, i,j].unsqueeze(-1)\n",
        "\n",
        "\n",
        "                # All paths to out tag \n",
        "                transition_to_tag = self.transition_matrix[:,j].unsqueeze(0)\n",
        "\n",
        "                # Adding rather than multplying as we are in log space\n",
        "\n",
        "                prob_i_j = prob_tag + transition_to_tag + alpha_iter\n",
        "                store[:, j] = torch.logsumexp(prob_i_j , dim = 1)\n",
        "            alpha_iter = store\n",
        "        # Finally add the transition form any to <EOT>\n",
        "        alpha_iter += self.transition_matrix[:,tag2index[\"<EOT>\"]]\n",
        "        return torch.logsumexp(alpha_iter , dim = 1)\n",
        "\n",
        "    def partition_optimized(self, predicted , targets):  # optimized form of calc_partition_func\n",
        "        # The difference here is that we perform all the computation in matrix format\n",
        "        # This makes this code faster on a GPU\n",
        "        batch_size , seq_len , _ = predicted.size()\n",
        "        n_labels = self.n_labels \n",
        "     \n",
        "        # I store all the alphas values as we need all of them to compute gradient\n",
        "        # This is a miniute but important detail.\n",
        "        alpha_iter = torch.empty(batch_size,seq_len, self.n_labels).to(device)\n",
        "         \n",
        "        alpha_iter[:,0,:] = predicted[:,0,:] + self.transition_matrix[tag2index[\"<BOT>\"],:].unsqueeze(0)\n",
        "\n",
        "        # We can finish all the computation in one loop only\n",
        "        for i in range(1, seq_len):\n",
        "            # This does exactely what the calc_partition_func does.\n",
        "            store = self.transition_matrix.unsqueeze(0) + alpha_iter[:,i-1,:].unsqueeze(-1) + predicted[:,i,:].unsqueeze(1)\n",
        "            alpha_iter[:,i,:] = torch.logsumexp(store , dim = 1)\n",
        "        \n",
        "        alpha_iter[:,seq_len -1, :] += self.transition_matrix[:,tag2index[\"<EOT>\"]]\n",
        "        return torch.logsumexp(alpha_iter[:,seq_len-1,:] , dim = 1)\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otq7JQwyAChx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now onto the main model (This will use the CRF model)\n",
        "\n",
        "class Tagger(nn.Module):\n",
        "    def __init__(self , n_labels, lstm_hidden_dim , embedding_dim , lstm_layers):\n",
        "        super().__init__()\n",
        "        self.n_labels = n_labels\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_layers = lstm_layers\n",
        "        # We need a trainable word embedding (Could have used pre trained also :))\n",
        "        self.embedding = nn.Embedding(self.embedding_dim[0], self.embedding_dim[1])\n",
        "\n",
        "        # Now initializing out Bi-LSTM layer\n",
        "        self.LSTM = nn.LSTM(input_size= self.embedding_dim[1] , hidden_size= self.lstm_hidden_dim , batch_first = True , bidirectional = True , num_layers = lstm_layers)\n",
        "\n",
        "        # We also need a linear layer to translate BiLSTMs retults to proper shape (probs for all tags)\n",
        "        self.linear1 = nn.Linear(self.lstm_hidden_dim*2 , self.n_labels)\n",
        "\n",
        "        # Now lets use the CRF class we worte before\n",
        "        self.CRF = CRF_linear(self.n_labels)\n",
        "    def forward(self, X):\n",
        "        X = self.embedding(X)  # Shape of X becomes (bs , seq_len , embedding_dim)\n",
        "        X, _ = self.LSTM(X)     # shape of X becomes (bs, seq_len , lstm_hidden_dim*2)\n",
        "        X = self.linear1(X)   # shape of X becomes (bs , seq_len , n_labels)\n",
        "        return X\n",
        "\n",
        "    def loss(self, X,y):\n",
        "        predictions = self.forward(X)\n",
        "        loss = self.CRF(predictions , y)\n",
        "        return loss\n",
        "    \n",
        "    def Infer(self, X):\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            predicted = self.forward(X)\n",
        "            transition_matrix = self.CRF.transition_matrix.data\n",
        "        batch_size , seq_len ,_ = predicted.size()\n",
        "        # Applying the Viterbi algo for finding best path\n",
        "        # We must maintain both alpha and a backpointer for this\n",
        "        alpha_iter = predicted[:,0,:] + transition_matrix[tag2index[\"<BOT>\"],:].unsqueeze(0)\n",
        "        back_pointers = torch.empty(batch_size , seq_len -1 , self.n_labels , dtype= torch.long)\n",
        "\n",
        "\n",
        "        # The following for loop calculates the best probablity back pointers.\n",
        "        # This is an optimization so that maximum calculation are done in matrix format\n",
        "        for i in range(1,seq_len):\n",
        "            store = transition_matrix.unsqueeze(0) + alpha_iter.unsqueeze(-1) + predicted[:,i,:].unsqueeze(1)\n",
        "            alpha_iter , back_pointers[: , i-1, :]      =  torch.max(store , dim = 1)\n",
        "    \n",
        "        alpha_iter += transition_matrix[:,tag2index[\"<EOT>\"]]\n",
        "\n",
        "        # Getting the last max score and the index corrosponding to it\n",
        "        max_path_score ,max_last = torch.max(alpha_iter ,dim = 1)\n",
        "        return self.get_best_path(back_pointers ,max_last ,batch_size , seq_len)\n",
        "\n",
        "    def get_best_path(self, back_pointers , max_last , batch_size , seq_len):\n",
        "        # Here we finally assemble the path/answer using the back pointers\n",
        "        path = torch.empty(batch_size ,seq_len , dtype = torch.long)\n",
        "        path[:,-1] = max_last\n",
        "        for i in range(seq_len- 2 , -1 ,-1):\n",
        "            path[:,i] = torch.gather(input = back_pointers[:,i,:] , dim =1 , index = path[:,i+1].unsqueeze(-1)).squeeze()\n",
        "\n",
        "        return path\n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGc9_-DNN2vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Tagger(n_labels= len(tag2index) , lstm_hidden_dim= 100 , embedding_dim= (len(word2index), 140) , lstm_layers= 2)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGJYhchZ3mo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = .001\n",
        "optimizer = torch.optim.Adam(list(model.parameters()), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mWUUshlPYh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets train the model\n",
        "def train(model , epochs , batch_size):\n",
        "    inner_loop_size = len(train_data)//batch_size\n",
        "    for _ in range(epochs):\n",
        "        loss_arr = []\n",
        "        epoch_loss = 0\n",
        "        for i in range(inner_loop_size):\n",
        "            X,y = get_train_batch(train_data ,batch_size)\n",
        "            loss = model.loss(X,y).mean()  \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            epoch_loss += loss.item()\n",
        "        print(epoch_loss/inner_loop_size)\n",
        "        loss_arr.append(epoch_loss/inner_loop_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaNHbCC-UMei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "4a105c32-47b7-4ac3-f021-31fe0f0f438c"
      },
      "source": [
        "train(model ,10 , 10) "
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10050016147943476\n",
            "0.08728446180024467\n",
            "0.08599331826486914\n",
            "0.08365354076271884\n",
            "0.08023602541406688\n",
            "0.08254415404744397\n",
            "0.0728342134136172\n",
            "0.07544207415248697\n",
            "0.07019999147085987\n",
            "0.06912186529461242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQJ69ux8ooO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets take our model for a test run !!!\n",
        "class test_data_feeder:\n",
        "    \"\"\"\n",
        "    This class feeds the test_data example by example \n",
        "    \"\"\"\n",
        "    def __init__(self,test_data): \n",
        "        self.test_data = test_data\n",
        "        self.current_index = 0\n",
        "    def get_next(self):\n",
        "        if(self.current_index >= len(test_data)):\n",
        "            return None\n",
        "        \n",
        "        nxt = np.array(self.test_data[self.current_index])\n",
        "        X_test = torch.tensor(nxt[:,0]).to(device)\n",
        "        y_test = nxt[:,1]\n",
        "        self.current_index = self.current_index + 1\n",
        "        return X_test, y_test\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXMwzZ-w-6Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets see an example \n",
        "test_loader = test_data_feeder(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIX4HLNC_WRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test,y_test = test_loader.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuCe43DEB0Zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4670daa3-c0a9-4bf3-fca1-441e7e83003d"
      },
      "source": [
        "# First lets see the correct sequence of outputs\n",
        "print(y_test.tolist())"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-S7QIi8B00e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20b69193-9017-4dee-a755-60066bd75cbc"
      },
      "source": [
        "# Now lets see what my model predicts\n",
        "model.Infer(X_test.unsqueeze(0))[0].numpy().tolist()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfspxJepB2t2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Well Well what do you know the model correctely predicted the entire sequence !!!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUH0z9myDRsd",
        "colab_type": "text"
      },
      "source": [
        "**Lets put it all to test !!! **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcm5GPaQDU7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_array = []\n",
        "correct_array = []\n",
        "# now running the model for all the test examples !!!!\n",
        "def tester():\n",
        "     while(True):\n",
        "        obj = test_loader.get_next()\n",
        "        if(obj == None):  # We we have finished iterating through the test data\n",
        "            break \n",
        "        \n",
        "        X_test , y_test = obj\n",
        "        global predicted_array\n",
        "        global correct_array\n",
        "        predicted = model.Infer(X_test.unsqueeze(0))[0].numpy().tolist()\n",
        "        predicted_array = predicted_array +  predicted\n",
        "        correct_array += y_test.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD6-yd0BEXTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tester()  # Running tester will fill the predicted and correct arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le11fqRBEYsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1 = f1_score(correct_array, predicted_array, average=\"macro\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnD82TfxEa2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd1105f5-6ac1-4b0a-e728-af7b2d3f2a11"
      },
      "source": [
        "f1"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5898674736952072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI6VOaR-JjQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision = precision_score(correct_array, predicted_array, average=\"macro\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIsAe9NGSYUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61b5d763-3213-459f-a494-77abcff7f227"
      },
      "source": [
        "precision"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6346489731214398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qll0M8GrcgUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}